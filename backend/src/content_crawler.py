import asyncio
from datetime import datetime
from typing import Optional
import trafilatura
from sqlalchemy.orm import Session

from .database import SessionLocal
from .models import Article as DBArticle


def fetch_article_content(url: str) -> tuple[Optional[str], Optional[datetime]]:
    """
    StÃ¡hne a rozparsuje ÄlÃ¡nek pomocÃ­ Trafilatura.
    VracÃ­ tuple (markdown_content, published_date).
    """
    try:
        print(f"ğŸ“° Stahuji ÄlÃ¡nek: {url}")
        
        # StaÅ¾enÃ­ HTML
        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            print(f"   âŒ NepodaÅ™ilo se stÃ¡hnout URL")
            return None, None
        
        # Extrakce obsahu s metadaty
        metadata = trafilatura.extract_metadata(downloaded)
        content = trafilatura.extract(
            downloaded,
            output_format='markdown',
            include_comments=False,
            include_tables=True
        )
        
        if not content or len(content.strip()) < 100:
            print(f"   âš ï¸ PÅ™Ã­liÅ¡ mÃ¡lo obsahu ({len(content) if content else 0} znakÅ¯)")
            return None, None
        
        # ZÃ­skÃ¡nÃ­ data vydÃ¡nÃ­ z metadat
        published_date = None
        if metadata and metadata.date:
            try:
                published_date = datetime.fromisoformat(metadata.date)
            except:
                pass
        
        print(f"   âœ“ StaÅ¾eno {len(content)} znakÅ¯")
        return content, published_date
        
    except Exception as e:
        print(f"   âŒ Chyba pÅ™i stahovÃ¡nÃ­ ÄlÃ¡nku {url}: {e}")
        return None, None


def process_articles(db: Session) -> dict:
    """
    Projde vÅ¡echny ÄlÃ¡nky v databÃ¡zi a doplnÃ­ jejich obsah.
    Pokud ÄlÃ¡nek uÅ¾ obsah mÃ¡, pÅ™epÃ­Å¡e ho.
    """
    articles = db.query(DBArticle).all()
    
    stats = {
        "total": len(articles),
        "success": 0,
        "failed": 0,
        "skipped": 0
    }
    
    print(f"\nğŸ”„ ZpracovÃ¡vÃ¡m {stats['total']} ÄlÃ¡nkÅ¯...")
    
    for i, article in enumerate(articles, 1):
        print(f"\n[{i}/{stats['total']}] {article.title[:60]}...")
        
        # Fetch obsahu
        content, published_date = fetch_article_content(article.url)
        
        if content:
            # UloÅ¾enÃ­ do databÃ¡ze (pÅ™epÃ­Å¡e existujÃ­cÃ­ obsah)
            article.content = content
            article.published_date = published_date
            stats["success"] += 1
        else:
            stats["failed"] += 1
        
        # Commit po kaÅ¾dÃ©m ÄlÃ¡nku (aby se neztratila data pÅ™i pÃ¡du)
        try:
            db.commit()
        except Exception as e:
            print(f"   âŒ Chyba pÅ™i uklÃ¡dÃ¡nÃ­: {e}")
            db.rollback()
            stats["failed"] += 1
            stats["success"] -= 1
    
    return stats


async def main():
    """
    HlavnÃ­ funkce content crawleru.
    """
    print("="*60)
    print("ğŸš€ Content Crawler Worker")
    print("="*60)
    
    db = SessionLocal()
    try:
        stats = process_articles(db)
        
        print("\n" + "="*60)
        print("âœ… VÃSLEDEK")
        print("="*60)
        print(f"Celkem ÄlÃ¡nkÅ¯: {stats['total']}")
        print(f"ÃšspÄ›Å¡nÄ› staÅ¾eno: {stats['success']}")
        print(f"Selhalo: {stats['failed']}")
        
    finally:
        db.close()


if __name__ == "__main__":
    asyncio.run(main())
