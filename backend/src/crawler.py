import asyncio
import os
import json
from typing import List, Optional
from dotenv import load_dotenv

from playwright.async_api import async_playwright
from langchain_google_genai import ChatGoogleGenerativeAI
from pydantic import BaseModel, Field

from .database import SessionLocal
from .models import Article as DBArticle

# 1. NaÄtenÃ­ API klÃ­Äe
load_dotenv()

TARGET_URL = os.getenv("TARGET_URL", "https://www.novinky.cz")

# 2. Definice datovÃ½ch modelÅ¯ (Vstup a VÃ½stup pro AI)
class LinkItem(BaseModel):
    text: str
    url: str

    # PÄ›knÃ½ vÃ½pis pro debugovÃ¡nÃ­
    def __repr__(self):
        return f"[{self.text[:20]}...] -> {self.url}"

class ArticleItem(BaseModel):
    """
    Jeden vybranÃ½ ÄlÃ¡nek s kategorizacÃ­.
    """
    index: int = Field(description="Index ÄlÃ¡nku ze vstupnÃ­ho seznamu (0-based)")
    countries: List[str] = Field(
        description="Seznam zemÃ­, kterÃ½ch se ÄlÃ¡nek tÃ½kÃ¡ (napÅ™. ÄŒesko, NÄ›mecko, USA, EU). PrÃ¡zdnÃ½ seznam pokud se netÃ½kÃ¡ konkrÃ©tnÃ­ zemÄ›."
    )
    people: List[str] = Field(
        description="Seznam veÅ™ejnÃ½ch osob (jmÃ©no nebo funkce), kterÃ½ch se ÄlÃ¡nek tÃ½kÃ¡. PrÃ¡zdnÃ½ seznam pokud se netÃ½kÃ¡ konkrÃ©tnÃ­ osoby."
    )

class ArticleSelection(BaseModel):
    """
    Toto je struktura, kterou chceme, aby nÃ¡m AI vrÃ¡tila.
    LangChain zajistÃ­, Å¾e dostaneme pÅ™esnÄ› tento formÃ¡t (JSON).
    """
    articles: List[ArticleItem] = Field(
        description="Seznam vybranÃ½ch ÄlÃ¡nkÅ¯ s jejich indexy a kategorizacÃ­."
    )

# 3. NastavenÃ­ AI (Gemini 1.5 Flash)
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-lite",
    temperature=0,
)

# PÅ™ipojÃ­me schÃ©ma vÃ½stupu k modelu
ai_selector = llm.with_structured_output(ArticleSelection)

async def get_page_links(url: str) -> List[LinkItem]:
    """
    PomocÃ­ Playwright stÃ¡hne odkazy, vyÄistÃ­ je a odstranÃ­ duplicity.
    """
    async with async_playwright() as p:
        print(f"ğŸŒ NaÄÃ­tÃ¡m strÃ¡nku: {url}")
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        try:
            await page.goto(url, wait_until="domcontentloaded")
            
            # JavaScript v prohlÃ­Å¾eÄi pro rychlou extrakci
            raw_data = await page.evaluate("""() => {
                return Array.from(document.querySelectorAll('a')).map(a => ({
                    text: a.innerText.replace(/[\\n\\t]/g, ' ').trim(), // OdstranÄ›nÃ­ odÅ™Ã¡dkovÃ¡nÃ­
                    url: a.href
                }));
            }""")
            
        finally:
            await browser.close()

    print(f"ğŸ” Nalezeno surovÃ½ch odkazÅ¯: {len(raw_data)}")

    # --- Python Filtrace (Cleaning) ---
    unique_map = {}
    
    for item in raw_data:
        text = item['text']
        url = item['url']
        
        # 1. MusÃ­ to bÃ½t http(s) odkaz
        if not url.startswith("http"):
            continue
            
        # 2. MusÃ­ mÃ­t text (netextovÃ© odkazy zahazujeme)
        #    Ignorujeme i velmi krÃ¡tkÃ© texty (ÄÃ­sla strÃ¡nek, "vÃ­ce", atd.)
        if not text or len(text) < 5:
            continue
            
        # 3. Dedupikace (podle URL)
        # Pokud uÅ¾ URL mÃ¡me, ale tenhle novÃ½ odkaz mÃ¡ delÅ¡Ã­ text (lepÅ¡Ã­ kontext), pÅ™epÃ­Å¡eme ho
        if url not in unique_map:
            unique_map[url] = LinkItem(text=text, url=url)
        else:
            if len(text) > len(unique_map[url].text):
                unique_map[url] = LinkItem(text=text, url=url)

    clean_links = list(unique_map.values())
    print(f"ğŸ§¹ Po vyÄiÅ¡tÄ›nÃ­ a deduplikaci zbÃ½vÃ¡: {len(clean_links)} odkazÅ¯.")
    return clean_links


async def analyze_with_ai(links: List[LinkItem]) -> List[ArticleItem]:
    """
    PoÅ¡le seznam odkazÅ¯ do Gemini k posouzenÃ­.
    VrÃ¡tÃ­ seznam ÄlÃ¡nkÅ¯ s indexy a kategorizacÃ­.
    """
    if not links:
        return []
    
    print("ğŸ¤– PosÃ­lÃ¡m data agentovi k analÃ½ze...")
    
    # Prompt (Instrukce pro agenta)
    # VytvoÅ™Ã­me indexovanÃ½ seznam nadpisÅ¯
    indexed_titles = "\n".join([f"{i}. {link.text}" for i, link in enumerate(links)])
    
    prompt_text = (
        "Jsi redakÄnÃ­ robot. TvÃ½m Ãºkolem je projÃ­t indexovanÃ½ seznam nadpisÅ¯ z hlavnÃ­ strÃ¡nky zpravodajskÃ©ho webu "
        "a vybrat POUZE ty, kterÃ© jsou **konkrÃ©tnÃ­ ÄlÃ¡nky** (zprÃ¡vy, reportÃ¡Å¾e, komentÃ¡Å™e).\n\n"
        "PRAVIDLA:\n"
        "1. VYBER nadpisy, kterÃ© vypadajÃ­ jako titulky ÄlÃ¡nkÅ¯.\n"
        "2. IGNORUJ navigaÄnÃ­ odkazy (DomÅ¯, Sport, PoÄasÃ­, AutoÅ™i, Archiv).\n"
        "3. IGNORUJ patiÄku, reklamu, login a technickÃ© strÃ¡nky.\n"
        "4. Pro kaÅ¾dÃ½ vybranÃ½ ÄlÃ¡nek urÄi:\n"
        "   - **zemÄ›**: kterÃ© se ÄlÃ¡nek tÃ½kÃ¡ (ÄŒesko, NÄ›mecko, USA, EU, atd.). Pokud se tÃ½kÃ¡ EU jako celku, uveÄ 'EU'.\n"
        "   - **osoby**: veÅ™ejnÃ© osoby (jmÃ©no nebo funkce), kterÃ½ch se ÄlÃ¡nek tÃ½kÃ¡.\n\n"
        "VraÅ¥ indexy vybranÃ½ch ÄlÃ¡nkÅ¯ (0-based) spolu s kategorizacÃ­.\n\n"
        f"Seznam nadpisÅ¯:\n{indexed_titles}"
    )

    # print(prompt_text)
    
    try:
        result = await ai_selector.ainvoke(prompt_text)
        return result.articles
    except Exception as e:
        print(f"âŒ Chyba pÅ™i komunikaci s AI: {e}")
        return []


def save_to_database(articles: List[ArticleItem], links: List[LinkItem]) -> None:
    """
    SmaÅ¾e databÃ¡zi a uloÅ¾Ã­ novÃ© ÄlÃ¡nky.
    """
    db = SessionLocal()
    try:
        # 1. SmazÃ¡nÃ­ vÅ¡ech existujÃ­cÃ­ch ÄlÃ¡nkÅ¯
        print("\nğŸ—‘ï¸  MaÅ¾u starÃ© ÄlÃ¡nky z databÃ¡ze...")
        deleted_count = db.query(DBArticle).delete()
        print(f"   SmazÃ¡no: {deleted_count} ÄlÃ¡nkÅ¯")
        
        # 2. UloÅ¾enÃ­ novÃ½ch ÄlÃ¡nkÅ¯
        print("\nğŸ’¾ UklÃ¡dÃ¡m novÃ© ÄlÃ¡nky do databÃ¡ze...")
        for article in articles:
            if 0 <= article.index < len(links):
                link = links[article.index]
                
                # VytvoÅ™enÃ­ kategorizace jako JSON string
                categories_data = {
                    "countries": article.countries,
                    "people": article.people
                }
                
                db_article = DBArticle(
                    title=link.text,
                    url=link.url,
                    categories=json.dumps(categories_data, ensure_ascii=False)
                )
                db.add(db_article)
        
        db.commit()
        print(f"   âœ… UloÅ¾eno: {len(articles)} ÄlÃ¡nkÅ¯")
        
    except Exception as e:
        db.rollback()
        print(f"   âŒ Chyba pÅ™i uklÃ¡dÃ¡nÃ­ do databÃ¡ze: {e}")
        raise
    finally:
        db.close()


async def main():
    # 1. Krok: ZÃ­skÃ¡nÃ­ dat
    links = await get_page_links(TARGET_URL)
    
    # 2. Krok: AnalÃ½za AI
    # Pro jistotu vezmeme prvnÃ­ch 50 nejdelÅ¡Ã­ch odkazÅ¯ (ÄlÃ¡nky mÃ­vajÃ­ dlouhÃ© titulky),
    # abychom neplatili za analÃ½zu menu a patiÄek zbyteÄnÄ›.
    # (SeÅ™adÃ­me podle dÃ©lky textu sestupnÄ›)
    sorted_links = sorted(links, key=lambda x: len(x.text), reverse=True)
    top_candidates = sorted_links[:50]
    
    articles = await analyze_with_ai(top_candidates)

    # 3. Krok: UloÅ¾enÃ­ do databÃ¡ze
    save_to_database(articles, top_candidates)

    # 4. Krok: VÃ½pis
    print("\n" + "="*60)
    print(f"âœ… VÃSLEDEK: Nalezeno {len(articles)} ÄlÃ¡nkÅ¯")
    print("="*60)
    
    for i, article in enumerate(articles, 1):
        # Rekonstrukce nadpisu podle indexu
        if 0 <= article.index < len(top_candidates):
            title = top_candidates[article.index].text
        else:
            title = "âš ï¸ NeplatnÃ½ index"
        
        print(f"\n{i:02d}. {title}")
        
        # VÃ½pis kategoriÃ­
        if article.countries:
            print(f"    ğŸŒ ZemÄ›: {', '.join(article.countries)}")
        if article.people:
            print(f"    ğŸ‘¤ Osoby: {', '.join(article.people)}")

if __name__ == "__main__":
    asyncio.run(main())