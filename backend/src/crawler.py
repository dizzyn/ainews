import asyncio
import os
import json
from typing import List, Optional
from dotenv import load_dotenv

from playwright.async_api import async_playwright
from langchain_google_genai import ChatGoogleGenerativeAI
from pydantic import BaseModel, Field

from .database import SessionLocal
from .models import Article as DBArticle

# 1. Naƒçten√≠ API kl√≠ƒçe a konfigurace
load_dotenv()

TARGET_URL = os.getenv("TARGET_URL", "https://www.novinky.cz")
MAX_ARTICLES = int(os.getenv("MAX_ARTICLES", "100"))
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "20"))

# 2. Definice datov√Ωch model≈Ø (Vstup a V√Ωstup pro AI)
class LinkItem(BaseModel):
    text: str
    url: str

    # Pƒõkn√Ω v√Ωpis pro debugov√°n√≠
    def __repr__(self):
        return f"[{self.text[:20]}...] -> {self.url}"

class ArticleItem(BaseModel):
    """
    Jedna vybran√° zpr√°va s kategorizac√≠.
    """
    index: int = Field(
        description="Index zpr√°vy ze vstupn√≠ho seznamu (0-based)",
        ge=0
    )
    what_happened: str = Field(
        description="V kr√°tk√© vƒõtƒõ: co se stalo, co d≈ô√≠ve nebylo a teƒè je, jak√° nov√° informace byla zji≈°tƒõna.",
        min_length=10,
        max_length=500
    )
    impact_on: str = Field(
        description="Na koho m√° ud√°lost dopad - jednotlivec, skupina, organizace, st√°t, atd.",
        min_length=5,
        max_length=300
    )
    countries: List[str] = Field(
        default_factory=list,
        description="Seznam zem√≠, kter√Ωch se zpr√°va t√Ωk√° (nap≈ô. ƒåesko, Nƒõmecko, USA, EU). Pr√°zdn√Ω seznam pokud se net√Ωk√° konkr√©tn√≠ zemƒõ.",
        max_length=10
    )
    people: List[str] = Field(
        default_factory=list,
        description="Seznam ve≈ôejn√Ωch osob (jm√©no nebo funkce), kter√Ωch se zpr√°va t√Ωk√°. Pr√°zdn√Ω seznam pokud se net√Ωk√° konkr√©tn√≠ osoby.",
        max_length=20
    )

class ArticleSelection(BaseModel):
    """
    Toto je struktura, kterou chceme, aby n√°m AI vr√°tila.
    LangChain zajist√≠, ≈æe dostaneme p≈ôesnƒõ tento form√°t (JSON).
    """
    articles: List[ArticleItem] = Field(
        description="Seznam vybran√Ωch zpr√°v s jejich indexy a kategorizac√≠."
    )

# 3. Nastaven√≠ AI (Gemini 1.5 Flash)
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-lite",
    temperature=0,
)

# P≈ôipoj√≠me sch√©ma v√Ωstupu k modelu
ai_selector = llm.with_structured_output(ArticleSelection)

async def get_page_links(url: str) -> List[LinkItem]:
    """
    Pomoc√≠ Playwright st√°hne odkazy, vyƒçist√≠ je a odstran√≠ duplicity.
    """
    async with async_playwright() as p:
        print(f"üåç Naƒç√≠t√°m str√°nku: {url}")
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        try:
            await page.goto(url, wait_until="domcontentloaded")
            
            # JavaScript v prohl√≠≈æeƒçi pro rychlou extrakci
            raw_data = await page.evaluate("""() => {
                return Array.from(document.querySelectorAll('a')).map(a => ({
                    text: a.innerText.replace(/[\\n\\t]/g, ' ').trim(), // Odstranƒõn√≠ od≈ô√°dkov√°n√≠
                    url: a.href
                }));
            }""")
            
        finally:
            await browser.close()

    print(f"üîé Nalezeno surov√Ωch odkaz≈Ø: {len(raw_data)}")

    # --- Python Filtrace (Cleaning) ---
    unique_map = {}
    
    for item in raw_data:
        text = item['text']
        url = item['url']
        
        # 1. Mus√≠ to b√Ωt http(s) odkaz
        if not url.startswith("http"):
            continue
            
        # 2. Mus√≠ m√≠t text (netextov√© odkazy zahazujeme)
        #    Ignorujeme i velmi kr√°tk√© texty (ƒç√≠sla str√°nek, "v√≠ce", atd.)
        if not text or len(text) < 5:
            continue
            
        # 3. Dedupikace (podle URL)
        # Pokud u≈æ URL m√°me, ale tenhle nov√Ω odkaz m√° del≈°√≠ text (lep≈°√≠ kontext), p≈ôep√≠≈°eme ho
        if url not in unique_map:
            unique_map[url] = LinkItem(text=text, url=url)
        else:
            if len(text) > len(unique_map[url].text):
                unique_map[url] = LinkItem(text=text, url=url)

    clean_links = list(unique_map.values())
    print(f"üßπ Po vyƒçi≈°tƒõn√≠ a deduplikaci zb√Ωv√°: {len(clean_links)} odkaz≈Ø.")
    return clean_links


async def analyze_chunk_with_ai(links: List[LinkItem], chunk_offset: int = 0) -> List[ArticleItem]:
    """
    Po≈°le jeden chunk odkaz≈Ø do Gemini k posouzen√≠.
    Vr√°t√≠ seznam ƒçl√°nk≈Ø s indexy a kategorizac√≠.
    chunk_offset se p≈ôiƒç√≠t√° k index≈Øm pro spr√°vn√© mapov√°n√≠ na celkov√Ω seznam.
    """
    if not links:
        return []
    
    print(f"ü§ñ Analyzuji chunk {chunk_offset}-{chunk_offset + len(links) - 1}...")
    
    # Vytvo≈ô√≠me indexovan√Ω seznam nadpis≈Ø (s lok√°ln√≠mi indexy)
    indexed_titles = "\n".join([f"{i}. {link.text}" for i, link in enumerate(links)])
    
    prompt_text = (
        "You are an editorial robot. Your task is to review an indexed list of headlines from a news website's main page "
        "and select ONLY those that are **news with informational value**.\n\n"
        "CRITERIA FOR SELECTING NEWS:\n"
        "A news item must meet BOTH of the following criteria:\n"
        "1. Something NEW happened or we learned something that was not previously known\n"
        "2. The reported event has an IMPACT on someone (individual, group, organization, state)\n\n"
        "WHAT TO EXCLUDE:\n"
        "- Navigation links (Home, Sports, Weather, Authors, Archive)\n"
        "- Footer, advertisements, login, and technical pages\n"
        "- General articles without a specific event (tips, guides, product reviews)\n"
        "- Comments and analyses without a new event (look for prefixes like 'koment√°≈ô', 'point of view', or similar)\n"
        "- Sports results and entertainment news (unless they have broader social impact)\n"
        "- Jokes and artistic content (in Czech: 'vtip', 'umƒõn√≠') - these may look like articles but are entertainment/art content\n"
        "- Opinion pieces and commentaries - some commentators write their findings but it's not news (recognizable by 'koment√°≈ô', 'point of view', or similar prefixes)\n\n"
        "FOR EACH SELECTED NEWS ITEM, DETERMINE:\n"
        "1. **what_happened**: In a short sentence, summarize what happened - what was not there before and is now\n"
        "2. **impact_on**: Who is affected by the event (e.g., 'citizens of Czech Republic', 'employees of company X', 'patients', 'Donald Trump')\n"
        "3. **countries**: List of countries the news relates to (Czech Republic, Germany, USA, EU, etc.)\n"
        "4. **people**: List of public figures (name or position) the news relates to\n\n"
        "Return the indices of selected news items (0-based) along with complete categorization.\n\n"
        f"List of headlines:\n{indexed_titles}"
    )
    
    try:
        result = await ai_selector.ainvoke(prompt_text)
        # P≈ôiƒçteme offset k index≈Øm pro spr√°vn√© mapov√°n√≠
        for article in result.articles:
            article.index += chunk_offset
        return result.articles
    except Exception as e:
        print(f"‚ùå Chyba p≈ôi komunikaci s AI: {e}")
        return []


async def analyze_with_ai_in_chunks(links: List[LinkItem], chunk_size: int = CHUNK_SIZE) -> List[ArticleItem]:
    """
    Rozdƒõl√≠ odkazy na men≈°√≠ chunky a zpracuje je postupnƒõ.
    Vr√°t√≠ agregovan√Ω seznam v≈°ech vybran√Ωch ƒçl√°nk≈Ø.
    """
    if not links:
        return []
    
    all_articles = []
    total_chunks = (len(links) + chunk_size - 1) // chunk_size
    
    print(f"\nüì¶ Zpracov√°v√°m {len(links)} odkaz≈Ø v {total_chunks} chunc√≠ch po {chunk_size}...")
    
    for i in range(0, len(links), chunk_size):
        chunk = links[i:i + chunk_size]
        chunk_num = i // chunk_size + 1
        print(f"\n--- Chunk {chunk_num}/{total_chunks} ---")
        
        articles = await analyze_chunk_with_ai(chunk, chunk_offset=i)
        all_articles.extend(articles)
        
        print(f"   ‚úì Nalezeno {len(articles)} zpr√°v v tomto chunku")
    
    print(f"\n‚úÖ Celkem nalezeno {len(all_articles)} zpr√°v ze v≈°ech chunk≈Ø")
    return all_articles


def save_to_database(articles: List[ArticleItem], links: List[LinkItem]) -> None:
    """
    Sma≈æe datab√°zi a ulo≈æ√≠ nov√© zpr√°vy.
    """
    db = SessionLocal()
    try:
        # 1. Smaz√°n√≠ v≈°ech existuj√≠c√≠ch ƒçl√°nk≈Ø
        print("\nüóëÔ∏è  Ma≈æu star√© zpr√°vy z datab√°ze...")
        deleted_count = db.query(DBArticle).delete()
        print(f"   Smaz√°no: {deleted_count} zpr√°v")
        
        # 2. Ulo≈æen√≠ nov√Ωch zpr√°v
        print("\nüíæ Ukl√°d√°m nov√© zpr√°vy do datab√°ze...")
        for article in articles:
            if 0 <= article.index < len(links):
                link = links[article.index]
                
                # Vytvo≈ôen√≠ kategorizace jako JSON string
                categories_data = {
                    "what_happened": article.what_happened,
                    "impact_on": article.impact_on,
                    "countries": article.countries,
                    "people": article.people
                }
                
                db_article = DBArticle(
                    title=link.text,
                    url=link.url,
                    categories=json.dumps(categories_data, ensure_ascii=False)
                )
                db.add(db_article)
        
        db.commit()
        print(f"   ‚úÖ Ulo≈æeno: {len(articles)} zpr√°v")
        
    except Exception as e:
        db.rollback()
        print(f"   ‚ùå Chyba p≈ôi ukl√°d√°n√≠ do datab√°ze: {e}")
        raise
    finally:
        db.close()


async def main():
    # 1. Krok: Z√≠sk√°n√≠ dat
    links = await get_page_links(TARGET_URL)
    
    # 2. Krok: P≈ô√≠prava kandid√°t≈Ø
    # Se≈ôad√≠me podle d√©lky textu sestupnƒõ (ƒçl√°nky m√≠vaj√≠ dlouh√© titulky)
    # a vezmeme prvn√≠ch MAX_ARTICLES kandid√°t≈Ø
    sorted_links = sorted(links, key=lambda x: len(x.text), reverse=True)
    top_candidates = sorted_links[:MAX_ARTICLES]
    
    print(f"\nüìä Zpracov√°v√°m {len(top_candidates)} kandid√°t≈Ø (MAX_ARTICLES={MAX_ARTICLES})")
    
    # 3. Krok: Anal√Ωza AI po chunc√≠ch
    articles = await analyze_with_ai_in_chunks(top_candidates, chunk_size=CHUNK_SIZE)

    # 4. Krok: Ulo≈æen√≠ do datab√°ze
    save_to_database(articles, top_candidates)

    # 5. Krok: V√Ωpis
    print("\n" + "="*60)
    print(f"‚úÖ V√ùSLEDEK: Nalezeno {len(articles)} zpr√°v")
    print("="*60)
    
    for i, article in enumerate(articles, 1):
        # Rekonstrukce nadpisu podle indexu
        if 0 <= article.index < len(top_candidates):
            title = top_candidates[article.index].text
        else:
            title = "‚ö†Ô∏è Neplatn√Ω index"
        
        print(f"\n{i:02d}. {title}")
        
        # V√Ωpis nov√Ωch klasifikac√≠
        print(f"    üì∞ Co se stalo: {article.what_happened}")
        print(f"    üéØ Dopad na: {article.impact_on}")
        
        # V√Ωpis p≈Øvodn√≠ch kategori√≠
        if article.countries:
            print(f"    üåç Zemƒõ: {', '.join(article.countries)}")
        if article.people:
            print(f"    üë§ Osoby: {', '.join(article.people)}")

if __name__ == "__main__":
    asyncio.run(main())